{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quickstart & Sanity Checks\n",
    "Use this notebook to confirm required datasets exist and preview a few rows per domain before running flows." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path('..').resolve()\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "print('Project root:', project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected raw data paths\n",
    "Update this list if you use different filenames." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "paths = {\n",
    "    'fraud': project_root / 'data/raw/fraud/creditcard.csv',\n",
    "    'cyber': project_root / 'data/raw/cyber/kitsune_mirai.csv',\n",
    "    'behavior': project_root / 'data/raw/behavior/online_shoppers_intention.csv',\n",
    "    'nlp_enron': project_root / 'data/raw/nlp/enron_emails.csv',\n",
    "    'nlp_fakenews_dir': project_root / 'data/raw/nlp/fakenews',\n",
    "    'vision_root': project_root / 'data/raw/vision',\n",
    "}\n",
    "missing = {k: p for k, p in paths.items() if not p.exists()}\n",
    "print('Missing paths:' if missing else 'All listed paths exist')\n",
    "pprint(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview tabular datasets (fraud, cyber, behavior)\n",
    "Guarded reads: skips if file not found." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def safe_head(path, n=5):\n",
    "    if not path.exists():\n",
    "        print(f'Missing: {path}')\n",
    "        return\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f'File: {path.name}, rows={len(df)}, cols={len(df.columns)}')\n",
    "        display(df.head(n))\n",
    "    except Exception as e:\n",
    "        print(f'Failed to read {path}:', e)\n",
    "\n",
    "safe_head(paths['fraud'])\n",
    "safe_head(paths['cyber'])\n",
    "safe_head(paths['behavior'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview NLP (Enron or Fake News)\n",
    "Shows a few texts if available." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uais.data.load_datasets import load_enron_emails\n",
    "try:\n",
    "    df_enron = load_enron_emails(subset=3)\n",
    "    display(df_enron.head(3))\n",
    "except Exception as e:\n",
    "    print('Enron load skipped:', e)\n",
    "\n",
    "fake_news_dir = paths['nlp_fakenews_dir']\n",
    "if fake_news_dir.exists():\n",
    "    fake_csv = list(fake_news_dir.rglob('Fake.csv'))\n",
    "    true_csv = list(fake_news_dir.rglob('True.csv'))\n",
    "    if fake_csv and true_csv:\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df_fake = pd.read_csv(fake_csv[0]).head(2)\n",
    "            df_true = pd.read_csv(true_csv[0]).head(2)\n",
    "            display(df_fake.head(2))\n",
    "            display(df_true.head(2))\n",
    "        except Exception as e:\n",
    "            print('Fake/True preview skipped:', e)\n",
    "else:\n",
    "    print('Fake news dataset not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview vision layout\n",
    "Lists a few subfolders/files (no heavy image load)." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_root = paths['vision_root']\n",
    "if vision_root.exists():\n",
    "    entries = sorted([p for p in vision_root.iterdir() if p.is_dir()])[:5]\n",
    "    print('Vision subdirs:', [e.name for e in entries])\n",
    "else:\n",
    "    print('Vision root not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: CIFAR-10 shape check (if downloaded via download_nlp_vision)" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from uais.data.load_datasets import load_cifar10\n",
    "    X, y = load_cifar10('train')\n",
    "    print('CIFAR-10 train shape:', X.shape, y.shape)\n",
    "except Exception as e:\n",
    "    print('CIFAR check skipped:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all CSVs per domain (quick head)\n",
    "Attempts to read every CSV under each domain folder and show the first few rows. Skips on errors/large files." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import islice\n",
    "domains_dirs = {\n",
    "    'fraud': project_root / 'data/raw/fraud',\n",
    "    'cyber': project_root / 'data/raw/cyber',\n",
    "    'behavior': project_root / 'data/raw/behavior',\n",
    "    'nlp': project_root / 'data/raw/nlp',\n",
    "    'vision': project_root / 'data/raw/vision',\n",
    "}\n",
    "for dom, root in domains_dirs.items():\n",
    "    print(f\"\\n== {dom.upper()} ==\")\n",
    "    if not root.exists():\n",
    "        print('missing root', root)\n",
    "        continue\n",
    "    csvs = list(root.rglob('*.csv'))\n",
    "    if not csvs:\n",
    "        print('no CSVs found')\n",
    "        continue\n",
    "    for csv_path in islice(csvs, 5):  # cap to 5 files to keep it light\n",
    "        try:\n",
    "            df_tmp = pd.read_csv(csv_path)\n",
    "            print(f\"{csv_path.name}: rows={len(df_tmp)}, cols={len(df_tmp.columns)}\")\n",
    "            display(df_tmp.head(2))\n",
    "        except Exception as e:\n",
    "            print(f\"skip {csv_path.name}: {e}\")\n",
    "    if len(csvs) > 5:\n",
    "        print(f\"...skipped {len(csvs)-5} more files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- If a dataset is missing, use the download scripts under `src/uais/data/`.\n",
    "- Re-run this notebook after adding data to confirm everything is in place." 
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
